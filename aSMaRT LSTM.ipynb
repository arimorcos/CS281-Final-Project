{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def adam_loves_theano(inp_list,cost,param_list,alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    adam: adaptive... momentum???\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp_list: List of Theano variables\n",
    "        Whatever non-parameter things are needed to do a training step\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "    param_list: List of Theano variables\n",
    "        The variables that are changed for optimization\n",
    "    [alpha]: {0.001}\n",
    "        Training parameter: learning rate\n",
    "    [beta1]: {0.9}\n",
    "        Training parameter: decay rate for momentum\n",
    "    [beta2]: {0.999}\n",
    "        Training parameter: decay rate for velocity\n",
    "    [epsilon]: {1e-7}\n",
    "        Training parameter: i dunno.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    2 functions, which take the same inputs and must be called sequentially:\n",
    "        f_adam_helpers (updates helpers)\n",
    "        f_adam_train (uses updated helpers to update parameters in param_list)\n",
    "\n",
    "    \"\"\"\n",
    "    # Create 2 theano functions that will be called sequentially\n",
    "    # The first one \"updates\" the shared variables that go into the calculation of the parameter update\n",
    "    # The second one combines them into an update\n",
    "    \n",
    "    # Create the first function:\n",
    "    # Initialize the helper variables, one for each parameter (this will only happen once and doesn't affect updates)\n",
    "    Ts = [theano.shared(0.)               for p in param_list] # t term in adam\n",
    "    Ms = [theano.shared(p.get_value()*0.) for p in param_list] # m term in adam\n",
    "    Vs = [theano.shared(p.get_value()*0.) for p in param_list] # v term in adam\n",
    "    # Define each of their update rules\n",
    "    up_t = [(T_,T_+1) for T_ in Ts]\n",
    "    up_m = [(M,beta1*M + (1-beta1)*T.grad(cost,p))      for M, p in zip(Ms,param_list)]\n",
    "    up_v = [(V,beta2*V + (1-beta2)*(T.grad(cost,p)**2)) for V, p in zip(Vs,param_list)]\n",
    "    # Combine this into a full update list\n",
    "    up_h = up_t + up_m + up_v\n",
    "    # Create that first function\n",
    "    f_adam_helpers = theano.function(inp_list,cost,updates=up_h)\n",
    "    \n",
    "    # Create the second function (during training, this is called right after calling the first):\n",
    "    # Compute, using the updated helper variables, the components of the parameter update equation\n",
    "    # (updated by the call to f_adam_helpers, which will occurr during training)\n",
    "    mHat = [m / (1-(beta1**t)) for m, t in zip(Ms,Ts)]\n",
    "    vHat = [v / (1-(beta2**t)) for v, t in zip(Vs,Ts)]\n",
    "    # Use them to update the parameters\n",
    "    up_p = [(p, p - (alpha*mH / (T.sqrt(vH)+epsilon))) for p, mH, vH in zip(param_list,mHat,vHat)]\n",
    "    # Create your training function with this update\n",
    "    f_adam_train = theano.function(inp_list,cost,updates=up_p)\n",
    "    \n",
    "    return f_adam_helpers, f_adam_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def adadelta_fears_committment(inp_list,cost,param_list,rho=.95, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    An adaptive learning rate optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp_list: List of Theano variables\n",
    "        Whatever non-parameter things are needed to do a training step\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "    param_list: List of Theano variables\n",
    "        The variables that are changed for optimization\n",
    "    [rho]: {0.95}\n",
    "        Training parameter: decay rate\n",
    "    [epsilon]: {1e-6}\n",
    "        Training parameter: i dunno.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    2 functions, which take the same inputs and must be called sequentially:\n",
    "        f_adadelta_helpers (updates helpers)\n",
    "        f_adadelta_train (uses updated helpers to update parameters in param_list)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [ADADELTA]_.\n",
    "\n",
    "    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n",
    "       Rate Method*, arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "\n",
    "    ### = DESCRIPTION FROM LITERATURE\n",
    "    \n",
    "    # Initialize the helper variables, one for each parameter (this will only happen once and doesn't affect updates)\n",
    "    # Standard gradients: g_t\n",
    "    zipped_grads   = [theano.shared(p.get_value()*np.zeros(1).astype(theano.config.floatX))\n",
    "                      for p in param_list]\n",
    "    # Running expectation of squared update: E[ d[x]**2 ]_t\n",
    "    running_up2    = [theano.shared(p.get_value()*np.zeros(1).astype(theano.config.floatX))\n",
    "                      for p in param_list]\n",
    "    # Running expectation of squared gradient: E[g**2]_t\n",
    "    running_grads2 = [theano.shared(p.get_value()*np.zeros(1).astype(theano.config.floatX))\n",
    "                      for p in param_list]\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Compute Gradient: g_t\n",
    "    # Update rule for shared variables in zipped_grads (they just equal variables in grads)\n",
    "    zgup = [(zg, T.grad(cost,p)) for zg, p in zip(zipped_grads, param_list)]\n",
    "    \n",
    "    ### Accumulate Gradient: E[g**2]_t = rho * E[g**2]_t-1  +  (1-rho) * (g_t)**2\n",
    "    # Update rule for shared variables in running_grads2\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (T.grad(cost,p) ** 2))\n",
    "             for rg2, p in zip(running_grads2, param_list)]\n",
    "\n",
    "    # Function that, when called, applies the two above update rules\n",
    "    # (during training, this is called, then f_update is)\n",
    "    f_adadelta_helpers = theano.function(inp_list,cost,updates=zgup+rg2up)\n",
    "\n",
    "    \n",
    "    ### Compute Update: d[x]_t = - [ RMS(d[x])_t-1 / RMS(g)_t ] * g_t\n",
    "    # Create symbolic variable out of zipped_grads, running_up2, and running_grads2 for each parameter\n",
    "    updir = [-T.sqrt(ru2 + epsilon) / T.sqrt(rg2 + epsilon) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    \n",
    "    ### Accumulate Update: E[ d[x]**2 ]_t = rho * E[ d[x]**2 ]_t-1  +  (1-rho) * (d[x]_t)**2\n",
    "    # Update rule for ru2up (whatever that is)\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2))\n",
    "             for ru2, ud in zip(running_up2, updir)]\n",
    "    \n",
    "    ### Apply Update: x_t+1 = x_t + d[x]_t\n",
    "    # Final update rule for parameter, combining all that\n",
    "    param_up = [(p, p + ud) for p, ud in zip(param_list, updir)]\n",
    "\n",
    "    # Function to actually update the parameters (as well as ru2up)\n",
    "    f_adadelta_train = theano.function(inp_list,cost, updates=ru2up + param_up)\n",
    "\n",
    "    return f_adadelta_helpers, f_adadelta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def i_hate_SGD(inp_list,cost,param_list,alpha=0.01):\n",
    "    \"\"\"\n",
    "    SGD: but why???\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp_list: List of Theano variables\n",
    "        Whatever non-parameter things are needed to do a training step\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "    param_list: List of Theano variables\n",
    "        The variables that are changed for optimization\n",
    "    [alpha]: {0.001}\n",
    "        Training parameter: learning rate\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    f_SGD_train: function\n",
    "        Uses updated helpers to update parameters in param_list\n",
    "\n",
    "    \"\"\"\n",
    "    # This is so straightforward I should punch you if you don't understand.\n",
    "    update_rules = [(p,p-T.grad(cost,p)*alpha) for p in param_list]\n",
    "    f_SGD_train = theano.function(inp_list,cost,updates=update_rules)\n",
    "    # Did you get it? Because if not you deserve punches.\n",
    "    return f_SGD_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Network Component Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_layer:\n",
    "    \"\"\"A layer of an LSTM network\"\"\"\n",
    "    def __init__(self,n_inp,n_hidden,n_out):\n",
    "        self.n_inp    = n_inp\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out    = n_out\n",
    "        # LSTM layers have, for every hidden \"unit\" a unit and a corresponding memory cell\n",
    "        # Memory cells include input, forget, and output gates as well as a value\n",
    "        # There is also a set of outputs. \n",
    "        # Fuck that's a lot of stuff.\n",
    "        # (this should help):\n",
    "        def init_w(n_in,n_out=n_hidden):\n",
    "            return theano.shared( np.random.uniform(\n",
    "                low = -1. / np.sqrt(n_in),\n",
    "                high = 1. / np.sqrt(n_in),\n",
    "                size = (n_out,n_in) ).astype(theano.config.floatX) )\n",
    "        def init_b(n=n_hidden):\n",
    "            return theano.shared( np.zeros(n).astype(theano.config.floatX) )\n",
    "        # Initialize attributes for every weight of i\n",
    "        self.w_i = init_w(n_inp+n_out + n_hidden + n_hidden) # (inp+prev_out + prev_hidden + prev_c)\n",
    "        self.b_i = init_b()\n",
    "        # Initialize attributes for every weight of f\n",
    "        self.w_f = init_w(n_inp + n_hidden + n_hidden) # (inp + prev_hidden + prev_c)\n",
    "        self.b_f = init_b()\n",
    "        # Initialize attributes for every weight of c\n",
    "        self.w_c = init_w(n_inp+n_out + n_hidden) # (inp+prev_out + prev_hidden)\n",
    "        self.b_c = init_b()\n",
    "        # Initialize attributes for every weight of o\n",
    "        self.w_o = init_w(n_inp+n_out + n_hidden + n_hidden) # (inp+prev_out + prev_hidden + CURRENT_c)\n",
    "        self.b_o = init_b()\n",
    "        # Intialize attributes for weights of y (the real output)\n",
    "        self.w_y = init_w(n_hidden,n_out)\n",
    "        self.b_y = init_b(n_out)\n",
    "        # Congrats. Now this is initialized.\n",
    "    \n",
    "    # Provide a list of all parameters to train\n",
    "    def list_params(self):\n",
    "        return [self.w_i,self.b_i,self.w_f,self.b_f,self.w_c,self.b_c,self.w_o,self.b_o,self.w_y,self.b_y]\n",
    "    \n",
    "    # Write methods for calculating the value of each of these playas at a given step\n",
    "    def calc_i(self,combined_inputs):\n",
    "        return T.nnet.sigmoid( T.dot( self.w_i, combined_inputs ) + self.b_i )\n",
    "    def calc_f(self,combined_inputs):\n",
    "        return T.nnet.sigmoid( T.dot( self.w_f, combined_inputs ) + self.b_f )\n",
    "    def calc_c(self,prev_c,curr_f,curr_i,combined_inputs):\n",
    "        return curr_f*prev_c + curr_i*T.tanh( T.dot( self.w_c, combined_inputs ) + self.b_c )\n",
    "    def calc_o(self,combined_inputs):\n",
    "        return T.nnet.sigmoid( T.dot( self.w_o, combined_inputs ) + self.b_o )\n",
    "    def calc_h(self,curr_o,curr_c):\n",
    "        return curr_o * T.tanh( curr_c )\n",
    "    def calc_y(self,curr_h):\n",
    "        return T.dot( self.w_y, curr_h ) + self.b_y\n",
    "    # Put this together in a method for updating c, h, and y\n",
    "    def step(self, inp, prev_c, prev_h, prev_y):\n",
    "        i = self.calc_i( T.concatenate((inp,prev_y,prev_h,prev_c)) )\n",
    "        f = self.calc_f( T.concatenate((inp,prev_h,prev_c)) )\n",
    "        c = self.calc_c( prev_c, f, i, T.concatenate((inp,prev_y,prev_h)) )\n",
    "        o = self.calc_o( T.concatenate((inp,prev_y,prev_h,c)) )\n",
    "        h = self.calc_h( o, c )\n",
    "        y = self.calc_y( h )\n",
    "        return c, h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_stack:\n",
    "    \"\"\"A stack of LSTMs\"\"\"\n",
    "    def __init__(self,inp_dim,layer_spec_list):\n",
    "        # Create each layer. Store them as a list.\n",
    "        self.layers = []\n",
    "        for K,spec in enumerate(layer_spec_list):\n",
    "            if K==0: my_inps = inp_dim\n",
    "            else:    my_inps = layer_spec_list[K-1][1]\n",
    "            self.layers = self.layers + [LSTM_layer(my_inps,spec[0],spec[1])]\n",
    "            \n",
    "    def list_params(self):\n",
    "        # Return all the parameters in this stack.... You sure?\n",
    "        P = []\n",
    "        for L in self.layers: P = P + L.list_params()\n",
    "        return P\n",
    "            \n",
    "    def process(self,inp_sequence):\n",
    "        # Go through the whole input and return the concatenated outputs of the stack after it's all said and done\n",
    "        outs = []\n",
    "        for K,layer in enumerate(self.layers):\n",
    "            if K==0: curr_seq = inp_sequence\n",
    "            else:    curr_seq = Y # (from previous layer)\n",
    "            out_init = [\n",
    "                T.alloc( np.zeros(1).astype(theano.config.floatX), layer.n_hidden),\n",
    "                T.alloc( np.zeros(1).astype(theano.config.floatX), layer.n_hidden),\n",
    "                T.alloc( np.zeros(1).astype(theano.config.floatX), layer.n_out)\n",
    "                ]\n",
    "            ([C,H,Y],updates) = theano.scan(fn = layer.step,\n",
    "                                            sequences = curr_seq,\n",
    "                                            outputs_info = out_init)\n",
    "            outs = outs + [Y[-1]]\n",
    "        return T.concatenate( tuple(outs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class soft_reader:\n",
    "    \"\"\"A softmax layer\"\"\"\n",
    "    def __init__(self,n_in,n_out):\n",
    "        # This is a simple layer, described just by a single weight matrix (no bias)\n",
    "        self.w = theano.shared( np.random.uniform(\n",
    "                low = -1. / np.sqrt(n_in),\n",
    "                high = 1. / np.sqrt(n_in),\n",
    "                size = (n_out,n_in) ).astype(theano.config.floatX) )\n",
    "    \n",
    "    def list_params(self):\n",
    "        # Easy.\n",
    "        return [self.w]\n",
    "    \n",
    "    def process(self,inp):\n",
    "        # Do your soft max kinda thing.\n",
    "        return T.nnet.softmax(T.dot(self.w,inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# The Network Kingpin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class full_net:\n",
    "    \"\"\"The full input to output network\"\"\"\n",
    "    def __init__(self,inp_dim,LSTM_spec_list,final_out_size):\n",
    "        # Get you your LSTM stack\n",
    "        self.LSTM_stack = LSTM_stack(inp_dim,LSTM_spec_list)\n",
    "        LSTM_out_size = 0\n",
    "        for L in LSTM_spec_list: LSTM_out_size += L[1]\n",
    "        # Get you your softmax readout\n",
    "        self.soft_reader = soft_reader(LSTM_out_size,final_out_size)\n",
    "        \n",
    "        ### ARTICULATE THE NETWORK GRAPH ###\n",
    "        # Input is a sequence represented by a matrix\n",
    "        inpSeq = T.dmatrix('inp')\n",
    "        # Output is a scalar indicating the correct answer\n",
    "        target = T.iscalar('target')\n",
    "        \n",
    "        # Through the LSTM stack, then soft max\n",
    "        y = self.LSTM_stack.process(inpSeq)\n",
    "        p = self.soft_reader.process(y)[0]\n",
    "        \n",
    "        # Give this class a process function\n",
    "        self.process = theano.function([inpSeq],p)\n",
    "        \n",
    "        # Cost is based on the probability given to the correct answer\n",
    "        # (this is like cross-entropy and still involves the whole w_v matrix because of softmax)\n",
    "        cost = -T.log(p[target])\n",
    "        ###\n",
    "        \n",
    "        ### For creating easy functions ###\n",
    "        self.__p = p\n",
    "        self.__cost = cost\n",
    "        self.__inp_list = [inpSeq,target]\n",
    "        self.__param_list = self.LSTM_stack.list_params() + self.soft_reader.list_params()\n",
    "        # For just getting your cost on a training example\n",
    "        self.cost = theano.function(self.__inp_list, self.__cost)\n",
    "        # For making training functions\n",
    "        self.__f_adam_helpers, self.__f_adam_train =\\\n",
    "            adam_loves_theano(self.__inp_list, self.__cost, self.__param_list) #adam\n",
    "        self.__f_adadelta_helpers, self.__f_adadelta_train =\\\n",
    "            adadelta_fears_committment(self.__inp_list, self.__cost, self.__param_list) #adadelta\n",
    "        \n",
    "    # These functions implements that sequential calling into one trianing step:\n",
    "    def adam_step(self,S,T):\n",
    "        self.__f_adam_helpers(S,T)\n",
    "        return self.__f_adam_train(S,T)\n",
    "    def adadelta_step(self,S,T):\n",
    "        self.__f_adadelta_helpers(S,T)\n",
    "        return self.__f_adadelta_train(S,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spec_list = 5*np.ones(shape=(2,2)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '46910' (I am process '47579')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '46910' (I am process '47579')\n"
     ]
    }
   ],
   "source": [
    "network = full_net(5, spec_list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSTM_stack',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '_full_net__cost',\n",
       " '_full_net__f_adadelta_helpers',\n",
       " '_full_net__f_adadelta_train',\n",
       " '_full_net__f_adam_helpers',\n",
       " '_full_net__f_adam_train',\n",
       " '_full_net__inp_list',\n",
       " '_full_net__p',\n",
       " '_full_net__param_list',\n",
       " 'adadelta_step',\n",
       " 'adam_step',\n",
       " 'cost',\n",
       " 'process',\n",
       " 'soft_reader']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.random.normal(size=(5,5))\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20543645,  0.19191193,  0.19097494,  0.20620995,  0.20546673])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.process(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
