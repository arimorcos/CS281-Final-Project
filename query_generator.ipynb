{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize as w_toke\n",
    "from nltk import sent_tokenize as s_toke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT CODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PIPELINE for ENTITY DETECTION / ANONYMIZATION\n",
    "#\n",
    "# (1) Create entries for all known entities\n",
    "# (2) Identify possibly ambiguous sub names\n",
    "# (3) Track possible sources of ambiguous sub names\n",
    "# (4) Create cleaned full text\n",
    "# (5) Replace known, unambiguous entities \n",
    "# (6) Replace ambiguous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (1) Create entries for all known entities\n",
    "# (2) Identify possibly ambiguous sub names\n",
    "# (3) Track possible sources of ambiguous sub names\n",
    "def init_entity_dict(char_dict):\n",
    "    \n",
    "    def clean_name(charName):\n",
    "        # Get rid of ...\n",
    "        # ... nickname indicator things\n",
    "        nuesance = (\"(?<= )'\",\"'(?= )\",'(?<= )\\\"','\\\"(?= )','\\(','\\)')\n",
    "        # ... seperators you sometimes see\n",
    "        nuesance = (\",(?= )\",\"(?<= )and(?= )\",\"(?<= )&(?= )\",\"(?<= )of(?= )\") + nuesance\n",
    "        # ... Common abbreviations for titles\n",
    "        nuesance = ('Mr\\.? ','Mrs\\.? ','Ms\\.? ','Dr\\.? ','Prof\\.? ') + nuesance\n",
    "        # ... Common titles\n",
    "        nuesance = ('The ','the ','Miss ','Master ','Mister ','Misses ','Doctor ','Professor ','Judge ') + nuesance\n",
    "        # ... Religious titles\n",
    "        nuesance = ('Father ','Bishop ','Saint ','Sister ') + nuesance\n",
    "        # ... Royalty\n",
    "        nuesance = ('King ','Queen ','Prince ','Princess ','Duke ','Dutchess ','Count ','Countess ','Baron ','Knight ','Lord ','Lady ') + nuesance\n",
    "        # ... Military rank\n",
    "        nuesance = ('Private ','Private First Class ','Corporal ','Sergeant ','Lieutenant ','Captain ','Major ','Colonel ','General ','Commodore ','Officer ') + nuesance\n",
    "\n",
    "        # Turn this into a regular expression search pattern\n",
    "        p = \"(\"\n",
    "        for i,x in enumerate(nuesance):\n",
    "            if i==len(nuesance)-1: p = p+x+\")\"\n",
    "            else: p = p+x+\"|\"\n",
    "        # Apply this first cleaning\n",
    "        charName = re.sub( p,'',charName)\n",
    "\n",
    "        # Address surrnames that show up as 2 words\n",
    "        earlyParts = ('De','de','Del','del','Von','von')\n",
    "        for ep in earlyParts:\n",
    "            # Look for a match and replace the gap with an underscore\n",
    "            charName = re.sub(ep+\" \\w+\",lambda x:re.sub(' ','_',x.group(0)),charName)\n",
    "\n",
    "        return charName \n",
    "    \n",
    "    def enmrate_namings(charName):\n",
    "        \"\"\"Get all the possible name usages, assuming that order is preserved\"\"\"\n",
    "        charName = re.sub('\\s*$','',charName)\n",
    "        # Apply whatever cleaning function you will use and convert to tokens\n",
    "        # Note: the original name does not get \"cleaned\"\n",
    "        subNames = w_toke(clean_name(charName))\n",
    "        # Our underscored connections (de_Jesus) survive tokenization, now we can get rid of them\n",
    "        subNames = [re.sub('_',' ',re.sub(' ','',str(x))) for x in list(subNames)]\n",
    "        \n",
    "        # Pull out all (however unlikely) name combos (within reason) that preserve name order\n",
    "        def buildNames(subNames,idxPairs):\n",
    "            return [ subNames[x[0]]+' '+subNames[x[1]] for x in idxPairs]\n",
    "        \n",
    "        if len(subNames) == 1:\n",
    "            indNames = [str(charName)]\n",
    "        if len(subNames) == 2:\n",
    "            indNames = [str(charName)] + subNames\n",
    "        if len(subNames) == 3:\n",
    "            indNames = [str(charName)] + subNames + buildNames(subNames,( (0,1),(0,2),(1,2) ))\n",
    "        if len(subNames) == 4:\n",
    "            indNames = [str(charName)] + subNames + buildNames(subNames,( (0,1),(0,2),(0,3),(1,2),(1,3),(2,3) ))\n",
    "        if len(subNames) >  4:\n",
    "            indNames = [str(charName)] + subNames # (fuck it)\n",
    "        return indNames\n",
    "\n",
    "    \n",
    "    # Intialize a dictionary for storing the entities.\n",
    "    eDict = {}\n",
    "    knownCount = 0\n",
    "    badCount = 0\n",
    "    for c in char_dict:\n",
    "        # Break apart any and all possible name usages, assuming that order is preserved and \"nicknames\" are never more than 3 sub names\n",
    "        allNames = enmrate_namings(c['name'])\n",
    "        # Step through each of these new names\n",
    "        origName = allNames[0]\n",
    "        for n in allNames:\n",
    "            # Does an entry already exist?\n",
    "            if eDict.has_key(n):\n",
    "                # We have found a conflict!\n",
    "                # Don't need to create a new entry, but update the list of source conflicts\n",
    "                eDict[n]['sources']  = eDict[n]['sources']  + [origName]\n",
    "                eDict[n]['members'] += 1\n",
    "                if n == origName:\n",
    "                    # Uh oh. A character's original name should lead to a new entry\n",
    "                    eDict[n]['orig_conflicts'] += 1\n",
    "                    if not eDict[n]['has_orig']:\n",
    "                        # I guess we should throw these in there for consistency\n",
    "                        knownCount += 1\n",
    "                        eDict[n]['has_orig'] = True\n",
    "                        eDict[n]['ent'] = 'ent%03d' % knownCount\n",
    "                        eDict[n]['last'] = 0\n",
    "                    # We also want to distinguish bad entities\n",
    "                    if not eDict[n].has_key('bEnt'):\n",
    "                        # Looks like we found a new one...\n",
    "                        badCount += 1\n",
    "                        eDict[n]['bEnt'] = 'bEnt%03d' % badCount\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                # This is a new name\n",
    "                # Create a new entity if it's an original\n",
    "                if n==origName:\n",
    "                    # It is an original\n",
    "                    knownCount += 1\n",
    "                    eDict[n] = {}\n",
    "                    eDict[n]['has_orig'] = True\n",
    "                    eDict[n]['ent'] = 'ent%03d' % knownCount\n",
    "                    eDict[n]['last'] = -1\n",
    "                else:\n",
    "                    # It is a possible subname, do not devote an entity\n",
    "                    eDict[n] = {}\n",
    "                    eDict[n]['has_orig'] = False\n",
    "                # These will be the same in either case\n",
    "                eDict[n]['sources'] = [origName]\n",
    "                eDict[n]['members'] = 1\n",
    "                # We need to flag/count catestrophic confusions\n",
    "                eDict[n]['orig_conflicts'] = 0\n",
    "        \n",
    "    # Make it easy to compare the level of entity badness\n",
    "    eCounts = {}\n",
    "    eCounts['TOT_ORIG_CONF'] = sum([eDict[x]['orig_conflicts'] for x in eDict.keys()])\n",
    "    eCounts['COUNTKNOWN'] = knownCount\n",
    "    eCounts['COUNTBAD']   = badCount\n",
    "        \n",
    "    return eDict, eCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The co-reference/substitution procedure\n",
    "def choose_entity_sub(mtchobj):\n",
    "    \"\"\"This function is called during a re.sub() routine as the replacement function\"\"\"\n",
    "    global eDict\n",
    "    name = mtchobj.group(0)\n",
    "    loc  = mtchobj.start(0)\n",
    "    # Resolve the root source (if it isn't resolved already)\n",
    "    if not eDict[name]['has_orig']:\n",
    "        # Not an original, but it might be unique\n",
    "        if eDict[name]['members'] == 1:\n",
    "            # Unique indeed. The source is our name.\n",
    "            source = eDict[name]['sources'][0]\n",
    "        else:\n",
    "            # It isn't unique, we need to resolve the name further...\n",
    "            # Assume that it refers to the most recently mentioned one.\n",
    "            # Sort the possible sources by most recent appearance\n",
    "            srtdSources = sorted( eDict[name]['sources'], key = lambda x: -eDict[x]['last'] )\n",
    "            # Take the source from the top of the list!\n",
    "            source = srtdSources[0]\n",
    "    else:\n",
    "        # It's an original name. So, there's your source\n",
    "        source = name\n",
    "\n",
    "    # We have resolved our source\n",
    "    if not eDict[source]['orig_conflicts']:\n",
    "        # We're good! Update that we saw this entity and return its code\n",
    "        eDict[source]['last'] = loc\n",
    "        return eDict[source]['ent']\n",
    "    else:\n",
    "        # This is too tricky to figure out. Return a bad-entity code\n",
    "        return eDict[source]['bEnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (4) Create cleaned full text\n",
    "# (5) Replace known, unambiguous entities \n",
    "# (6) Replace ambiguous entities\n",
    "def sub_known_entities(scraped_info,eDict,info_type):\n",
    "    \n",
    "    # Refresh the 'last' field\n",
    "    for x in eDict.keys():\n",
    "        if eDict[x].has_key('last'): eDict[x]['last'] = -1\n",
    "    \n",
    "    # List out all the possible known entities, sorted by length\n",
    "    E = ['(?<=\\W)'+re.escape(x)+'(?=\\W)' for x in sorted(eDict.keys(), key = lambda x: -len(x))]\n",
    "    # Create a greedy pattern lookup\n",
    "    p = \"(\"\n",
    "    for i,x in enumerate(E):\n",
    "        if i==len(E)-1: p = p+x+\")\"\n",
    "        else: p = p+x+\"|\"\n",
    "            \n",
    "    def clean_desc_start(thisChar):\n",
    "        # These first sentences of these seem to follow one of these formats:\n",
    "        #   Character Name, blah blah blah ...\n",
    "        #   Ethnic Character Name (PRO-nun-see-AY-shun), blah blah blah ...\n",
    "        # Turn it into:\n",
    "        #   Character Name is blah blah blah ...\n",
    "        thisName = re.sub('\\s*$','',thisChar['name']) # Remove lagging space\n",
    "        thisChar['description'] = re.sub(thisName+'\\s?(\\s*\\(.*\\))?,',thisName+' is',thisChar['description'])\n",
    "        return thisChar\n",
    "            \n",
    "    if info_type=='chars':\n",
    "        # Get the full character description\n",
    "        cleanChars = [ clean_desc_start(x) for x in scraped_info ]\n",
    "        # Pad the full description with spaces to make entities findable everywhere\n",
    "        fulldesc = ' '\n",
    "        for x in cleanChars: fulldesc = fulldesc + ' ' + x['description']\n",
    "        txt = fulldesc + ' '\n",
    "    elif info_type=='summary':\n",
    "        # Pad the summary with spaces to make entities findable everywhere\n",
    "        txt = ' ' + scraped_info + ' '\n",
    "\n",
    "    # Detect/replace the entities we know about\n",
    "    known_subbed_text = re.sub( p, choose_entity_sub, txt)\n",
    "    \n",
    "    # Return stuff\n",
    "    return known_subbed_text, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### THE QUERY GENERATION PIPELINE:\n",
    "#\n",
    "# For each book:\n",
    "#    Entity sub descriptions\n",
    "#    Store the dictionary used\n",
    "# For each available summary:\n",
    "#    Entity sub the summary\n",
    "#    Store anonymized summary\n",
    "#    Generate possible queries\n",
    "#    Prune crappy queries\n",
    "#    Enumerate queries\n",
    "#    Store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_maker(ks_desc,ks_summ):\n",
    "    \"\"\"Takes the known-substituted descriptions/summary and returns all the queries you can get from their combination\"\"\"\n",
    "    # Our quantum query is a sentence from the full description. Atomize it.\n",
    "    sents = s_toke(ks_desc)\n",
    "    # For each sentence, get a list of the unique entities mentioned\n",
    "    ents_mentioned = [set(re.findall('ent\\d\\d\\d',x)) for x in s_toke(ks_desc)]\n",
    "    # A key variable is how many entities are in each sentence\n",
    "    num_ents_in_sent = [len(x) for x in ents_mentioned]\n",
    "\n",
    "    # Simplify the ents_mentioned info by representing each entity with their ID number\n",
    "    def ent_to_id(entString):\n",
    "        return int(entString[3:])\n",
    "    def set_to_ids(ent_set):\n",
    "        return [ent_to_id(x) for x in list(ent_set)]\n",
    "    def mentions_to_id(ent_mentions):\n",
    "        return [set_to_ids(x) for x in ent_mentions]\n",
    "\n",
    "    # List of entities (by ID) mentioned in each sentence of the char descriptions\n",
    "    ids_mentioned = mentions_to_id( ents_mentioned )\n",
    "    # Count total mentions of each known entity in the summary\n",
    "    largest_ent = ent_to_id(max(set( re.findall('ent\\d\\d\\d',ks_desc) )))\n",
    "    tot_mentions = []\n",
    "    for e in range(largest_ent+1):\n",
    "        this_ent = 'ent%03d' % e\n",
    "        tot_mentions = tot_mentions + [len( re.findall( this_ent, ks_summ) )]\n",
    "\n",
    "    #For each sentence, how represented (in the summary) is the least represented entity\n",
    "    min_rep = []\n",
    "    for s in ids_mentioned:\n",
    "        mentions_of_relevance = [tot_mentions[x] for x in s]\n",
    "        if mentions_of_relevance: min_rep = min_rep + [min(mentions_of_relevance)]\n",
    "        else: min_rep = min_rep + [0]\n",
    "\n",
    "\n",
    "    # Now we have something to work with\n",
    "    possible_queries = zip(num_ents_in_sent,ids_mentioned,min_rep,sents)\n",
    "\n",
    "    # Cull shit we can't use\n",
    "    # Minimum representation (only keep queries if each entity is mentioned at least 3 times in the summary)\n",
    "    possible_queries = [x for x in possible_queries if x[2] >= 3]\n",
    "    \n",
    "    # Sort by representation density\n",
    "    possible_queries = sorted(possible_queries, key = lambda x: x[2])\n",
    "    \n",
    "    # Queries involving just 1 entity (likely to focus on plot)\n",
    "    possible_solo_queries = [x for x in possible_queries if x[0] == 1]\n",
    "    # Queries involving multiple entities\n",
    "    possible_mult_queries = [x for x in possible_queries if x[0] >= 2 and x[0] <= 4]\n",
    "\n",
    "    def make_queries(possible_queries):\n",
    "        # Now, we need to create the set of sentences that act as training queries for this summary\n",
    "        # Each sentence will contribute x times, where x is the number of unique entities it has\n",
    "        queries = []\n",
    "        answers = []\n",
    "        for p in possible_queries:\n",
    "            # Work with this sentence\n",
    "            this_sent = p[3]\n",
    "            # Step through each unique entity in the sentence\n",
    "            for e in p[1]:\n",
    "                # Put this guy in the answers\n",
    "                answers = answers + [e]\n",
    "                # Articulate the character representation\n",
    "                estr = 'ent%03d' % e\n",
    "                # Generate a query by replacing this entity with some marker in our sentence\n",
    "                this_query = re.sub(estr,'XXXXXX',this_sent)\n",
    "                # Add it to our list of queries\n",
    "                queries = queries + [this_query]\n",
    "        return queries, answers\n",
    "            \n",
    "    return make_queries(possible_solo_queries), make_queries(possible_mult_queries), possible_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE QUERIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the pandas dataframe with the complete raw data\n",
    "df = pd.read_pickle('complete_database.pd')\n",
    "# Shorthand the character descriptions and plot summaries\n",
    "C = df['characters']\n",
    "S2 = df['masterplots_ii_summary']\n",
    "S4 = df['masterplots_fourth_edition_summary']\n",
    "# Add the necessary fields to the table\n",
    "df['eDict'] = None\n",
    "df['anon_sum_ii'] = None\n",
    "df['anon_sum_iiii'] = None\n",
    "df['mult_queries_ii'] = None\n",
    "df['solo_queries_ii'] = None\n",
    "df['possible_queries_ii'] = None\n",
    "df['mult_queries_iiii'] = None\n",
    "df['solo_queries_iiii'] = None\n",
    "df['possible_queries_iiii'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the whole fucking pipeline in action!!!\n",
    "for BOOK in df.index:\n",
    "\n",
    "    # Initialize the dictionary\n",
    "    eDict, eCounts = init_entity_dict(C[BOOK])\n",
    "    # Store it\n",
    "    df['eDict'][BOOK] = (eDict, eCounts)\n",
    "    \n",
    "    # Get the full known-substituted and original character description summaries.\n",
    "    ks_desc,trsh = sub_known_entities( C[BOOK],eDict,'chars')\n",
    "    \n",
    "    # Try the first summary\n",
    "    if type(S2[BOOK])==float: # Silly, ari...\n",
    "        # Make this a more reasonable type. It's currently a NaN\n",
    "        S2[BOOK] = None\n",
    "    elif S2[BOOK]: # Probably good to go\n",
    "        # Get the full known-substituted and original summaries for MP2\n",
    "        ks_summ,trsh = sub_known_entities(S2[BOOK],eDict,'summary')\n",
    "        # Store it\n",
    "        df['anon_sum_ii'][BOOK] = ks_summ\n",
    "        # Generate the queries\n",
    "        solo_queries, mult_queries, possible_queries = query_maker(ks_desc,ks_summ)\n",
    "        # Store them\n",
    "        df['solo_queries_ii'][BOOK] = solo_queries\n",
    "        df['mult_queries_ii'][BOOK] = mult_queries\n",
    "        df['possible_queries_ii'][BOOK] = possible_queries\n",
    "        \n",
    "    # Try the second summary\n",
    "    if type(S4[BOOK])==float: # Silly, ari...\n",
    "        # Make this a more reasonable type. It's currently a NaN\n",
    "        S4[BOOK] = None\n",
    "    elif S4[BOOK]: # Probably good to go\n",
    "        # Get the full known-substituted and original summaries for MP2\n",
    "        ks_summ,trsh = sub_known_entities(S4[BOOK],eDict,'summary')\n",
    "        # Store it\n",
    "        df['anon_sum_iiii'][BOOK] = ks_summ\n",
    "        # Generate the queries\n",
    "        solo_queries, mult_queries, possible_queries = query_maker(ks_desc,ks_summ)\n",
    "        # Store them\n",
    "        df['solo_queries_iiii'][BOOK] = solo_queries\n",
    "        df['mult_queries_iiii'][BOOK] = mult_queries\n",
    "        df['possible_queries_iiii'][BOOK] = possible_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of   single-entity queries: 31030\n",
      "Total number of multiple-entity queries: 36903\n",
      "Total number of queries:                 67933\n"
     ]
    }
   ],
   "source": [
    "# Boom.\n",
    "\n",
    "all_solo_queries_ii   = [x[0] for x in df['solo_queries_ii']   if x]\n",
    "all_solo_queries_iiii = [x[0] for x in df['solo_queries_iiii'] if x]\n",
    "n_solo = 0\n",
    "for q in all_solo_queries_ii:   n_solo = n_solo + len(q)\n",
    "for q in all_solo_queries_iiii: n_solo = n_solo + len(q)\n",
    "    \n",
    "all_mult_queries_ii   = [x[0] for x in df['mult_queries_ii']   if x]\n",
    "all_mult_queries_iiii = [x[0] for x in df['mult_queries_iiii'] if x]\n",
    "n_mult = 0\n",
    "for q in all_mult_queries_ii:   n_mult = n_mult + len(q)\n",
    "for q in all_mult_queries_iiii: n_mult = n_mult + len(q)\n",
    "\n",
    "\n",
    "print 'Total number of   single-entity queries: {}'.format(n_solo)\n",
    "print 'Total number of multiple-entity queries: {}'.format(n_mult)\n",
    "print 'Total number of queries:                 {}'.format(n_solo+n_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(df,'database_with_queries.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
