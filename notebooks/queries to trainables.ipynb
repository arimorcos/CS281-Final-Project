{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.en import English, LOCAL_DATA_DIR\n",
    "data_dir = os.environ.get('SPACY_DATA', LOCAL_DATA_DIR)\n",
    "nlp = English(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_description(desc):\n",
    "    \"\"\"Puts final touches on cleaning the document to aid vectorization\"\"\"\n",
    "    \n",
    "    # This comes down to a series of regular expression cleanups.\n",
    "    \n",
    "    def reduce_mult_ent(desc):\n",
    "        def sub_fun(ent_pattern):\n",
    "            if ent_pattern.groups()[0] == ent_pattern.groups()[1]:\n",
    "                return ent_pattern.groups()[0]\n",
    "            else:\n",
    "                return ent_pattern.group()\n",
    "        desc = re.sub('\\(?(ent\\d\\d\\d)\\)?s*\\(?(ent\\d\\d\\d)\\)?',sub_fun,desc)\n",
    "        desc = re.sub('\\(?(ent\\d\\d\\d)\\)?\\s*\\(?(ent\\d\\d\\d)\\)?',sub_fun,desc)\n",
    "        desc = re.sub('\\(?(bEnt\\d\\d\\d)\\)?\\s*\\(?(bEnt\\d\\d\\d)\\)?',sub_fun,desc)\n",
    "        desc = re.sub('\\(?(bEnt\\d\\d\\d)\\)?\\s*\\(?(bEnt\\d\\d\\d)\\)?',sub_fun,desc)\n",
    "        return desc\n",
    "    \n",
    "    # Missing spaces after punctuation:\n",
    "    def fix_punc_after(text,punc):\n",
    "        search_pattern = '(?<=\\S)' + re.escape(punc) + '(?=[^\\'\\s])'\n",
    "        replace_with = punc + ' '\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    \n",
    "    # Missing spaces before punctuation:\n",
    "    def fix_punc_before(text,punc):\n",
    "        search_pattern = '(?<=[^\\'\\s])' + re.escape(punc) + '(?=(\\w|\\d|\\())'\n",
    "        replace_with = ' ' + punc\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    \n",
    "    # Missing spaces before and after punctuation:\n",
    "    def fix_punc_both(text,punc):\n",
    "        search_pattern = '(?<=(\\w|\\d|\\)|\\())' + re.escape(punc) + '(?=(\\w|\\d|\\)|\\())'\n",
    "        replace_with = ' ' + punc + ' '\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    \n",
    "    # Fix lonely punctuation\n",
    "    def lonely_punc_after(text,punc):\n",
    "        search_pattern = '\\s*' + re.escape(punc)\n",
    "        replace_with = punc\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    def lonely_punc_before(text,punc):\n",
    "        search_pattern = re.escape(punc) + '\\s*'\n",
    "        replace_with = punc\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    def lonely_punc_both(text,punc):\n",
    "        search_pattern = '\\s*' + re.escape(punc) + '\\s*'\n",
    "        replace_with = punc\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    \n",
    "    # Punctuation to quotation without spacation\n",
    "    def punc_to_quote(text,punc):\n",
    "        search_pattern = re.escape(punc+\"'\") + '(?=\\S)'\n",
    "        replace_with = punc + \"' \"\n",
    "        return re.sub( search_pattern, replace_with, text)\n",
    "    \n",
    "    \n",
    "    desc = reduce_mult_ent(desc)\n",
    "    \n",
    "    desc = lonely_punc_after(desc,'.')\n",
    "    desc = lonely_punc_after(desc,',')\n",
    "    desc = lonely_punc_after(desc,'!')\n",
    "    desc = lonely_punc_after(desc,'?')\n",
    "    desc = lonely_punc_after(desc,')')\n",
    "    \n",
    "    desc = lonely_punc_before(desc,'(')\n",
    "    \n",
    "    desc = punc_to_quote(desc,'.')\n",
    "    desc = punc_to_quote(desc,',')\n",
    "    desc = punc_to_quote(desc,'!')\n",
    "    desc = punc_to_quote(desc,'?')\n",
    "    desc = punc_to_quote(desc,')')\n",
    "    \n",
    "    desc = fix_punc_after(desc,'.')\n",
    "    desc = fix_punc_after(desc,',')\n",
    "    desc = fix_punc_after(desc,'!')\n",
    "    desc = fix_punc_after(desc,'?')\n",
    "    desc = fix_punc_after(desc,')')\n",
    "    \n",
    "    desc = fix_punc_before(desc,'(')\n",
    "    \n",
    "    desc = fix_punc_both(desc,'/')\n",
    "    desc = fix_punc_both(desc,'+')\n",
    "    desc = fix_punc_both(desc,'=')\n",
    "    desc = fix_punc_both(desc,'&')\n",
    "    \n",
    "    \n",
    "    desc = re.sub(re.escape('+'),'plus',desc)\n",
    "    desc = re.sub(re.escape('='),'equals',desc)\n",
    "    desc = re.sub(re.escape('&'),'and',desc)\n",
    "    desc = re.sub(re.escape('%'),'percent',desc)\n",
    "    desc = re.sub(re.escape('?'),'.',desc)\n",
    "    desc = re.sub(re.escape(':'),',',desc)\n",
    "    desc = re.sub('\\.+','.', desc)\n",
    "    \n",
    "    desc = re.sub(re.escape('\\.'),'.',desc)\n",
    "    desc = re.sub(re.escape('\\,'),',',desc)\n",
    "    \n",
    "    desc = re.sub(re.escape('Dr. '),'Doctor ',desc)\n",
    "    # spacy doesn't know these. Just remove them.\n",
    "    desc = re.sub(re.escape('Mr.'),'',desc)\n",
    "    desc = re.sub(re.escape('Mrs.'),'',desc)\n",
    "    \n",
    "    \n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want training examples to be accessible through a query list and document dictionary\n",
    "# Each document has a key to its own sub-dictionary\n",
    "#   Each sub-dictionary has the vectorized document, and a tag for each point in the sequence\n",
    "#   Tags say what to do at load time\n",
    "# The query list is a list of dictionaries where each element is a query\n",
    "#   Each dictionary has the vectorized query, tags for each point, the name of the reference document, and the answer\n",
    "# There must also be access to the vectors that are decided on by not spacy\n",
    "\n",
    "def vectorize_and_tag(text):\n",
    "    # Easy part: final scrub, tokenize, vectorize\n",
    "    clean_text = scrub_description(text)\n",
    "    tokenized_text = nlp(text)\n",
    "    vec_text = np.array( [x.vector for x in tokenized_text] )\n",
    "    \n",
    "    # Tags should correspond to one of the following:\n",
    "    #   Word spacy knows        -- no further action required\n",
    "    #   Word spacy doesn't know -- supply appropriate vector\n",
    "    #   Entity #X               -- give permuted good entity\n",
    "    #   Bad entity #X           -- give permuted bad entity\n",
    "    #   Query start tag         -- supply appropriate vector\n",
    "    #   Query blank tag         -- supply appropriate vector\n",
    "    tags = []\n",
    "    for t in tokenized_text:\n",
    "        \n",
    "        if t.vector_norm != 0.0:\n",
    "            # This is something spacy knew.\n",
    "            tags += [None]\n",
    "            \n",
    "        elif re.match('ent\\d\\d\\d',str(t)):\n",
    "            # This is a good entity. Supply that tag, and its ent#\n",
    "            entNum = int(str(t)[3:6])\n",
    "            tags += [(1,entNum)]\n",
    "            \n",
    "        elif re.match('bEnt\\d\\d\\d',str(t)):\n",
    "            # This is a bad entity. Supply that tag, and its bEnt#\n",
    "            bEntNum = int(str(t)[4:7])\n",
    "            tags += [(2,bEntNum)]\n",
    "            \n",
    "        elif re.match('QUERYSTART',str(t)):\n",
    "            tags += [(3,None)]\n",
    "            \n",
    "        elif re.match('XXXXXX',str(t)):\n",
    "            # Blank tag\n",
    "            tags += [(4,None)]\n",
    "            \n",
    "        else:\n",
    "            # Must be just some unknown word\n",
    "            tags += [(0,None)]\n",
    "    \n",
    "    return clean_text, vec_text, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_like_vecs(n,return_real_vecs=False):\n",
    "    \"\"\"Returns random vectors that inhabit the vector space of real names\"\"\"\n",
    "    \n",
    "    # Use these names to estimate a distribution describing where in vector space names live\n",
    "    string_of_names =\\\n",
    "        u'Alex Alice Brian Betty Carl Cindy David Dorothy Emilio Elizabeth France Francine Greg Gina' +\\\n",
    "        u'Harold Helen Ian Isabella Joshua Jane Kaleb Kimberly Leonard Lisa Mark Mindy Noah Nancy' +\\\n",
    "        u'Owen Olivia Paul Patsy Quinn Ronald Regina Steven Sally Teddy Tina Victor Vivian'\n",
    "    \n",
    "    # Process these names into their learned vectors\n",
    "    ndoc = nlp(string_of_names)\n",
    "    name_vecs = np.array([x.vector for x in ndoc if x.vector_norm != 0.0])\n",
    "    \n",
    "    # Generate names from the same empirical distribution\n",
    "    fake_vecs = np.random.multivariate_normal(\n",
    "        np.mean(name_vecs,axis=0),\n",
    "        np.cov(name_vecs.T)+.000001*np.eye(300), # (lambda*I provides full rank)\n",
    "        size=(n))\n",
    "    # Normalize them\n",
    "    fake_vecs = (fake_vecs.T / np.sqrt( np.sum( fake_vecs**2, axis=1 ) )).T\n",
    "    if return_real_vecs:\n",
    "        return fake_vecs, name_vecs\n",
    "    else:\n",
    "        return fake_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import FloatProgress\n",
    "from IPython.display import display\n",
    "def build_trainables(DF,num_ent_vecs,num_bEnt_vecs,vec_store_path):\n",
    "    doc_dic = {}\n",
    "    query_list = []\n",
    "    \n",
    "    doc_counter = 0\n",
    "    query_counter = 0\n",
    "    \n",
    "    # Generate vector lookups for good entities\n",
    "    ent_vecs = name_like_vecs(num_ent_vecs+1) # Add one because ent \"0\" is the blank\n",
    "    # Ensure that the closest 2 names are not too close\n",
    "    while np.max(( np.dot(ent_vecs,ent_vecs.T) - np.eye(num_ent_vecs+1) ).flatten()) > .9:\n",
    "        ent_vecs = name_like_vecs(num_ent_vecs+1)\n",
    "    \n",
    "    # Do the same thing for bad entity lookups\n",
    "    bEnt_vecs = name_like_vecs(num_bEnt_vecs)\n",
    "    # Ensure that the closest 2 names are not too close\n",
    "    while np.max(( np.dot(bEnt_vecs,bEnt_vecs.T) - np.eye(num_bEnt_vecs) ).flatten()) > .9:\n",
    "        bEnt_vecs = name_like_vecs(num+bEnt_vecs)\n",
    "    \n",
    "    # Step through each book and build the trainables\n",
    "    f = FloatProgress(min=0, max=DF.index[-1])\n",
    "    display(f)\n",
    "    for book in DF.index:\n",
    "        book_name = DF['title'][book]\n",
    "        \n",
    "        # Try each summary source\n",
    "        for S in [0,1]:\n",
    "            \n",
    "            if S==0: # Try source \"ii\"\n",
    "                if DF['anon_sum_ii'][book]:\n",
    "                    found = True\n",
    "                    # This summary exists. Create stuff for it.\n",
    "                    summ_name = re.sub('\\s','_',book_name) + '_ii'\n",
    "                    summ = DF['anon_sum_ii'][book]\n",
    "                    SQ = DF['solo_queries_ii'][book]\n",
    "                    MQ = DF['mult_queries_ii'][book]\n",
    "                else:\n",
    "                    found = False\n",
    "            \n",
    "            else: # Try source \"iiii\"\n",
    "                if DF['anon_sum_iiii'][book]:\n",
    "                    found = True\n",
    "                    # This summary exists. Create stuff for it.\n",
    "                    summ_name = re.sub('\\s','_',book_name) + '_iiii'\n",
    "                    summ = DF['anon_sum_iiii'][book]\n",
    "                    SQ = DF['solo_queries_iiii'][book]\n",
    "                    MQ = DF['mult_queries_iiii'][book]\n",
    "                else:\n",
    "                    found = False\n",
    "            \n",
    "            if found: # Data got.\n",
    "                # Tag whether the query is a solo or multiple\n",
    "                s_tag = ['s' for x in range(len(SQ[0]))]\n",
    "                m_tag = ['m' for x in range(len(MQ[0]))]\n",
    "                \n",
    "                # Merge the solo/mult tags, the queries, and the answers into their own lists\n",
    "                all_T = s_tag + m_tag\n",
    "                all_Q = SQ[0] + MQ[0]\n",
    "                all_A = SQ[1] + MQ[1]\n",
    "\n",
    "                # Vectors and tags for the summary\n",
    "                doc_dic[summ_name] = {}\n",
    "                doc_dic[summ_name]['text'], vecs, doc_dic[summ_name]['tags'] =\\\n",
    "                    vectorize_and_tag( summ )\n",
    "                \n",
    "                # NOTE: vecs do not go right into the dictionary.\n",
    "                # They get written to the hard drive and their address is stored as loc in the dictionary\n",
    "                doc_dic[summ_name]['loc'] = 'doc_%07d'%doc_counter + '.pkl'\n",
    "                fo = open(doc_dic[summ_name]['loc'],'wb')\n",
    "                cPickle.dump(vecs, fo)\n",
    "                fo.close()\n",
    "                doc_counter += 1\n",
    "                \n",
    "                # To leave in the option of pre-loading, have a key for vecs\n",
    "                doc_dic[summ_name]['vecs'] = None\n",
    "\n",
    "\n",
    "                # Step through each query and add it to the list\n",
    "                for t,q,a in zip(all_T,all_Q,all_A):\n",
    "                    # Create a dictionary for this query\n",
    "                    q_dic = {}\n",
    "                    q_dic['sm'] = t \n",
    "                    q_dic['a'] = a\n",
    "                    q_dic['doc'] = summ_name\n",
    "\n",
    "                    # Incorporate vectored query and token tags\n",
    "                    q_dic['text'], vecs, q_dic['tags'] = vectorize_and_tag( u'QUERYSTART ' + q )\n",
    "                    \n",
    "                    # NOTE: vecs do not go right into the dictionary.\n",
    "                    # They get written to the hard drive and their address is stored as loc in the dictionary\n",
    "                    q_dic['loc'] = 'query_%07d'%query_counter + '.pkl'\n",
    "                    fo = open(q_dic['loc'],'wb')\n",
    "                    cPickle.dump(vecs, fo)\n",
    "                    fo.close()\n",
    "                    query_counter += 1\n",
    "                    \n",
    "                    # To leave in the option of pre-loading, have a key for vecs\n",
    "                    q_dic['vecs'] = None\n",
    "                    \n",
    "                    # Add this query to the full query list\n",
    "                    query_list += [q_dic]\n",
    "    \n",
    "        f.value=book\n",
    "    \n",
    "    return doc_dic, query_list, ent_vecs, bEnt_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convert database into trainables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the database\n",
    "df = pd.read_pickle('./data/database_with_queries.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest-numbered entity: 150\n",
      "Largest-numbered BAD entity: 10\n"
     ]
    }
   ],
   "source": [
    "# This tells us what the largest numbered entity is\n",
    "max_ent = 'ent000'\n",
    "for AS in df['anon_sum_ii']:\n",
    "    if AS:\n",
    "        these_ents = re.findall('ent\\d\\d\\d',str(scrub_description(AS)))\n",
    "        if these_ents:\n",
    "            this_max = max(these_ents)\n",
    "        else:\n",
    "            this_max = 'ent000'\n",
    "        if this_max > max_ent:\n",
    "            max_ent = this_max\n",
    "for AS in df['anon_sum_iiii']:\n",
    "    if AS:\n",
    "        these_ents = re.findall('ent\\d\\d\\d',str(scrub_description(AS)))\n",
    "        if these_ents:\n",
    "            this_max = max(these_ents)\n",
    "        else:\n",
    "            this_max = 'ent000'\n",
    "        if this_max > max_ent:\n",
    "            max_ent = this_max\n",
    "\n",
    "# This tells us what the largest numbered BAD entity is\n",
    "max_bEnt = 'bEnt000'\n",
    "for AS in df['anon_sum_ii']:\n",
    "    if AS:\n",
    "        these_ents = re.findall('bEnt\\d\\d\\d',str(scrub_description(AS)))\n",
    "        if these_ents:\n",
    "            this_max = max(these_ents)\n",
    "        else:\n",
    "            this_max = 'bEnt000'\n",
    "        if this_max > max_bEnt:\n",
    "            max_bEnt = this_max\n",
    "for AS in df['anon_sum_iiii']:\n",
    "    if AS:\n",
    "        these_ents = re.findall('bEnt\\d\\d\\d',str(scrub_description(AS)))\n",
    "        if these_ents:\n",
    "            this_max = max(these_ents)\n",
    "        else:\n",
    "            this_max = 'bEnt000'\n",
    "        if this_max > max_bEnt:\n",
    "            max_bEnt = this_max\n",
    "\n",
    "            \n",
    "max_ent  = int(max_ent[3:6])\n",
    "max_bEnt = int(max_bEnt[4:7])\n",
    "print 'Largest-numbered entity: {}'.format(max_ent)\n",
    "print 'Largest-numbered BAD entity: {}'.format(max_bEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct it!\n",
    "save_path = '/Users/alex/Desktop/data manager/'\n",
    "doc_dic, query_list, ent_vecs, bEnt_vecs = build_trainables(\n",
    "    df,\n",
    "    max_ent,\n",
    "    max_bEnt*2,\n",
    "    save_path)\n",
    "\n",
    "# Save everything\n",
    "with open(save_path + 'document_dictionary.pkl','wb') as f:\n",
    "    cPickle.dump(doc_dic,f)\n",
    "with open(save_path + 'query_list.pkl','wb') as f:\n",
    "    cPickle.dump(query_list,f)\n",
    "with open(save_path + 'entity_vectors.pkl','wb') as f:\n",
    "    cPickle.dump(ent_vecs,f)\n",
    "with open(save_path + 'bad_entity_vectors.pkl','wb') as f:\n",
    "    cPickle.dump(bEnt_vecs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of document dictionary: 65.234588 MB\n",
      "Size of query list:          70.675218 MB\n"
     ]
    }
   ],
   "source": [
    "# Sizes of the resulting things\n",
    "D = 0\n",
    "for k,v in doc_dic.iteritems():\n",
    "    for k2,v2 in v.iteritems():\n",
    "        try:\n",
    "            nB = v2.nbytes\n",
    "        except:\n",
    "            nB = sys.getsizeof(v2)\n",
    "        D += nB\n",
    "Q = 0\n",
    "for q in query_list:\n",
    "    for k,v in q.iteritems():\n",
    "        try:\n",
    "            nB = v.nbytes\n",
    "        except:\n",
    "            nB = sys.getsizeof(v)\n",
    "        Q += nB\n",
    "\n",
    "print 'Size of document dictionary: {} MB'.format(D/1e6)\n",
    "print 'Size of query list:          {} MB'.format(Q/1e6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}