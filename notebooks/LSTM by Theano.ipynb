{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Build a tweet sentiment analyzer\n",
    "'''\n",
    "from collections import OrderedDict\n",
    "import cPickle as pkl\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = {'imdb': (imdb.load_data, imdb.prepare_data)}\n",
    "\n",
    "# Set the random number generators' seeds for consistency\n",
    "SEED = 123\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy_floatX(data):\n",
    "    return numpy.asarray(data, dtype=config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = numpy.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        numpy.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    return datasets[name][0], datasets[name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zipp(params, tparams):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    for kk, vv in params.iteritems():\n",
    "        tparams[kk].set_value(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(zipped):\n",
    "    \"\"\"\n",
    "    When we pickle the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.iteritems():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(state_before, use_noise, trng):\n",
    "    proj = tensor.switch(use_noise,\n",
    "                         (state_before *\n",
    "                          trng.binomial(state_before.shape,\n",
    "                                        p=0.5, n=1,\n",
    "                                        dtype=state_before.dtype)),\n",
    "                         state_before * 0.5)\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(options):\n",
    "    \"\"\"\n",
    "    Global (not LSTM) parameter. For the embeding and the classifier.\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "    # embedding\n",
    "    randn = numpy.random.rand(options['n_words'],\n",
    "                              options['dim_proj'])\n",
    "    params['Wemb'] = (0.01 * randn).astype(config.floatX)\n",
    "    params = get_layer(options['encoder'])[0](options,\n",
    "                                              params,\n",
    "                                              prefix=options['encoder'])\n",
    "    # classifier\n",
    "    params['U'] = 0.01 * numpy.random.randn(options['dim_proj'],\n",
    "                                            options['ydim']).astype(config.floatX)\n",
    "    params['b'] = numpy.zeros((options['ydim'],)).astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_params(path, params):\n",
    "    pp = numpy.load(path)\n",
    "    for kk, vv in params.iteritems():\n",
    "        if kk not in pp:\n",
    "            raise Warning('%s is not in the archive' % kk)\n",
    "        params[kk] = pp[kk]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.iteritems():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ortho_weight(ndim):\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, s, v = numpy.linalg.svd(W)\n",
    "    return u.astype(config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_init_lstm(options, params, prefix='lstm'):\n",
    "    \"\"\"\n",
    "    Init the LSTM parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    W = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'W')] = W\n",
    "    U = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'U')] = U\n",
    "    b = numpy.zeros((4 * options['dim_proj'],))\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_layer(tparams, state_below, options, prefix='lstm', mask=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "\n",
    "    def _step(m_, x_, h_, c_):\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')])\n",
    "        preact += x_\n",
    "\n",
    "        i = tensor.nnet.sigmoid(_slice(preact, 0, options['dim_proj']))\n",
    "        f = tensor.nnet.sigmoid(_slice(preact, 1, options['dim_proj']))\n",
    "        o = tensor.nnet.sigmoid(_slice(preact, 2, options['dim_proj']))\n",
    "        c = tensor.tanh(_slice(preact, 3, options['dim_proj']))\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n",
    "\n",
    "        h = o * tensor.tanh(c)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    dim_proj = options['dim_proj']\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=[tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj),\n",
    "                                              tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj)],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return rval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ff: Feed Forward (normal neural net), only useful to put after lstm\n",
    "#     before the classifier.\n",
    "layers = {'lstm': (param_init_lstm, lstm_layer)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(lr, tparams, grads, x, mask, y, cost):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    :note: A more complicated version of sgd then needed.  This is\n",
    "        done like that for adadelta and rmsprop.\n",
    "\n",
    "    \"\"\"\n",
    "    # New set of shared variable that will contain the gradient\n",
    "    # for a mini-batch.\n",
    "    gshared = [theano.shared(p.get_value() * 0., name='%s_grad' % k)\n",
    "               for k, p in tparams.iteritems()]\n",
    "    gsup = [(gs, g) for gs, g in zip(gshared, grads)]\n",
    "\n",
    "    # Function that computes gradients for a mini-batch, but do not\n",
    "    # updates the weights.\n",
    "    f_grad_shared = theano.function([x, mask, y], cost, updates=gsup,\n",
    "                                    name='sgd_f_grad_shared')\n",
    "\n",
    "    pup = [(p, p - lr * g) for p, g in zip(tparams.values(), gshared)]\n",
    "\n",
    "    # Function that updates the weights from the previously computed\n",
    "    # gradient.\n",
    "    f_update = theano.function([lr], [], updates=pup,\n",
    "                               name='sgd_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adadelta(lr, tparams, grads, x, mask, y, cost):\n",
    "    \"\"\"\n",
    "    An adaptive learning rate optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [ADADELTA]_.\n",
    "\n",
    "    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n",
    "       Rate Method*, arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "\n",
    "    ### = DESCRIPTION FROM LITERATURE\n",
    "    \n",
    "    # Initializes shared variables:\n",
    "    # Standard gradeints\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    # ???\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                 name='%s_rup2' % k)\n",
    "                   for k, p in tparams.iteritems()]\n",
    "    # ???\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    \n",
    "    ### Compute Gradient: g_t\n",
    "    # Update rule for shared variables in zipped_grads (they just equal variables in grads)\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    \n",
    "    ### Accumulate Gradient: E[g**2]_t = rho * E[g**2]_t-1  +  (1-rho) * (g_t)**2\n",
    "    # Update rule for shared variables in running_grads2\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    # Function that, when called, applies the two above update rules\n",
    "    # (during training, this is called, then f_update is)\n",
    "    f_grad_shared = theano.function([x, mask, y], cost, updates=zgup + rg2up,\n",
    "                                    name='adadelta_f_grad_shared')\n",
    "\n",
    "    \n",
    "    ### Compute Update: d[x]_t = - [ RMS(d[x])_t-1 / RMS(g)_t ] * g_t\n",
    "    # Create symbolic variable out of zipped_grads, running_up2, and running_grads2 for each parameter\n",
    "    updir = [-tensor.sqrt(ru2 + 1e-6) / tensor.sqrt(rg2 + 1e-6) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    \n",
    "    ### Accumulate Update: E[ d[x]**2 ]_t = rho * E[ d[x]**2 ]_t-1  +  (1-rho) * (d[x]_t)**2\n",
    "    # Update rule for ru2up (whatever that is)\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2))\n",
    "             for ru2, ud in zip(running_up2, updir)]\n",
    "    \n",
    "    ### Apply Update: x_t+1 = x_t + d[x]_t\n",
    "    # Final update rule for parameter, combining all that\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    # Function to actually update the parameters (as well as ru2up)\n",
    "    f_update = theano.function([lr], [], updates=ru2up + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsprop(lr, tparams, grads, x, mask, y, cost):\n",
    "    \"\"\"\n",
    "    A variant of  SGD that scales the step size by running average of the\n",
    "    recent step norms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [Hint2014]_.\n",
    "\n",
    "    .. [Hint2014] Geoff Hinton, *Neural Networks for Machine Learning*,\n",
    "       lecture 6a,\n",
    "       http://cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    running_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                   name='%s_rgrad' % k)\n",
    "                     for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rgup = [(rg, 0.95 * rg + 0.05 * g) for rg, g in zip(running_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([x, mask, y], cost,\n",
    "                                    updates=zgup + rgup + rg2up,\n",
    "                                    name='rmsprop_f_grad_shared')\n",
    "\n",
    "    updir = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                           name='%s_updir' % k)\n",
    "             for k, p in tparams.iteritems()]\n",
    "    updir_new = [(ud, 0.9 * ud - 1e-4 * zg / tensor.sqrt(rg2 - rg ** 2 + 1e-4))\n",
    "                 for ud, zg, rg, rg2 in zip(updir, zipped_grads, running_grads,\n",
    "                                            running_grads2)]\n",
    "    param_up = [(p, p + udn[1])\n",
    "                for p, udn in zip(tparams.values(), updir_new)]\n",
    "    f_update = theano.function([lr], [], updates=updir_new + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='rmsprop_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(tparams, options):\n",
    "    trng = RandomStreams(SEED)\n",
    "\n",
    "    # Used for dropout.\n",
    "    use_noise = theano.shared(numpy_floatX(0.))\n",
    "\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    mask = tensor.matrix('mask', dtype=config.floatX)\n",
    "    y = tensor.vector('y', dtype='int64')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps,\n",
    "                                                n_samples,\n",
    "                                                options['dim_proj']])\n",
    "    proj = get_layer(options['encoder'])[1](tparams, emb, options,\n",
    "                                            prefix=options['encoder'],\n",
    "                                            mask=mask)\n",
    "    if options['encoder'] == 'lstm':\n",
    "        proj = (proj * mask[:, :, None]).sum(axis=0)\n",
    "        proj = proj / mask.sum(axis=0)[:, None]\n",
    "    if options['use_dropout']:\n",
    "        proj = dropout_layer(proj, use_noise, trng)\n",
    "\n",
    "    pred = tensor.nnet.softmax(tensor.dot(proj, tparams['U']) + tparams['b'])\n",
    "\n",
    "    f_pred_prob = theano.function([x, mask], pred, name='f_pred_prob')\n",
    "    f_pred = theano.function([x, mask], pred.argmax(axis=1), name='f_pred')\n",
    "\n",
    "    off = 1e-8\n",
    "    if pred.dtype == 'float16':\n",
    "        off = 1e-6\n",
    "\n",
    "    cost = -tensor.log(pred[tensor.arange(n_samples), y] + off).mean()\n",
    "\n",
    "    return use_noise, x, mask, y, f_pred_prob, f_pred, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_probs(f_pred_prob, prepare_data, data, iterator, verbose=False):\n",
    "    \"\"\" If you want to use a trained model, this is useful to compute\n",
    "    the probabilities of new examples.\n",
    "    \"\"\"\n",
    "    n_samples = len(data[0])\n",
    "    probs = numpy.zeros((n_samples, 2)).astype(config.floatX)\n",
    "\n",
    "    n_done = 0\n",
    "\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, y = prepare_data([data[0][t] for t in valid_index],\n",
    "                                  numpy.array(data[1])[valid_index],\n",
    "                                  maxlen=None)\n",
    "        pred_probs = f_pred_prob(x, mask)\n",
    "        probs[valid_index, :] = pred_probs\n",
    "\n",
    "        n_done += len(valid_index)\n",
    "        if verbose:\n",
    "            print '%d/%d samples classified' % (n_done, n_samples)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_error(f_pred, prepare_data, data, iterator, verbose=False):\n",
    "    \"\"\"\n",
    "    Just compute the error\n",
    "    f_pred: Theano fct computing the prediction\n",
    "    prepare_data: usual prepare_data for that dataset.\n",
    "    \"\"\"\n",
    "    valid_err = 0\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, y = prepare_data([data[0][t] for t in valid_index],\n",
    "                                  numpy.array(data[1])[valid_index],\n",
    "                                  maxlen=None)\n",
    "        preds = f_pred(x, mask)\n",
    "        targets = numpy.array(data[1])[valid_index]\n",
    "        valid_err += (preds == targets).sum()\n",
    "    valid_err = 1. - numpy_floatX(valid_err) / len(data[0])\n",
    "\n",
    "    return valid_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    dim_proj=128,  # word embeding dimension and LSTM number of hidden units.\n",
    "    patience=10,  # Number of epoch to wait before early stop if no progress\n",
    "    max_epochs=5000,  # The maximum number of epoch to run\n",
    "    dispFreq=10,  # Display to stdout the training progress every N updates\n",
    "    decay_c=0.,  # Weight decay for the classifier applied to the U weights.\n",
    "    lrate=0.0001,  # Learning rate for sgd (not used for adadelta and rmsprop)\n",
    "    n_words=10000,  # Vocabulary size\n",
    "    optimizer=adadelta,  # sgd, adadelta and rmsprop available, sgd very hard to use, not recommanded (probably need momentum and decaying learning rate).\n",
    "    encoder='lstm',  # TODO: can be removed must be lstm.\n",
    "    saveto='lstm_model.npz',  # The best model will be saved there\n",
    "    validFreq=370,  # Compute the validation error after this number of update.\n",
    "    saveFreq=1110,  # Save the parameters after every saveFreq updates\n",
    "    maxlen=100,  # Sequence longer then this get ignored\n",
    "    batch_size=16,  # The batch size during training.\n",
    "    valid_batch_size=64,  # The batch size used for validation/test set.\n",
    "    dataset='imdb',\n",
    "\n",
    "    # Parameter for extra option\n",
    "    noise_std=0.,\n",
    "    use_dropout=True,  # if False slightly faster, but worst test error\n",
    "                       # This frequently need a bigger model.\n",
    "    reload_model=None,  # Path to a saved model we want to start from.\n",
    "    test_size=-1,  # If >0, we keep only this number of test example.\n",
    "):\n",
    "\n",
    "    # Model options\n",
    "    model_options = locals().copy()\n",
    "    print \"model options\", model_options\n",
    "\n",
    "    load_data, prepare_data = get_dataset(dataset)\n",
    "\n",
    "    print 'Loading data'\n",
    "    train, valid, test = load_data(n_words=n_words, valid_portion=0.05,\n",
    "                                   maxlen=maxlen)\n",
    "    if test_size > 0:\n",
    "        # The test set is sorted by size, but we want to keep random\n",
    "        # size example.  So we must select a random selection of the\n",
    "        # examples.\n",
    "        idx = numpy.arange(len(test[0]))\n",
    "        numpy.random.shuffle(idx)\n",
    "        idx = idx[:test_size]\n",
    "        test = ([test[0][n] for n in idx], [test[1][n] for n in idx])\n",
    "\n",
    "    ydim = numpy.max(train[1]) + 1\n",
    "\n",
    "    model_options['ydim'] = ydim\n",
    "\n",
    "    print 'Building model'\n",
    "    # This create the initial parameters as numpy ndarrays.\n",
    "    # Dict name (string) -> numpy ndarray\n",
    "    params = init_params(model_options)\n",
    "\n",
    "    if reload_model:\n",
    "        load_params('lstm_model.npz', params)\n",
    "\n",
    "    # This create Theano Shared Variable from the parameters.\n",
    "    # Dict name (string) -> Theano Tensor Shared Variable\n",
    "    # params and tparams have different copy of the weights.\n",
    "    tparams = init_tparams(params)\n",
    "\n",
    "    # use_noise is for dropout\n",
    "    (use_noise, x, mask,\n",
    "     y, f_pred_prob, f_pred, cost) = build_model(tparams, model_options)\n",
    "\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy_floatX(decay_c), name='decay_c')\n",
    "        weight_decay = 0.\n",
    "        weight_decay += (tparams['U'] ** 2).sum()\n",
    "        weight_decay *= decay_c\n",
    "        cost += weight_decay\n",
    "\n",
    "    f_cost = theano.function([x, mask, y], cost, name='f_cost')\n",
    "\n",
    "    grads = tensor.grad(cost, wrt=tparams.values())\n",
    "    f_grad = theano.function([x, mask, y], grads, name='f_grad')\n",
    "\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    f_grad_shared, f_update = optimizer(lr, tparams, grads,\n",
    "                                        x, mask, y, cost)\n",
    "\n",
    "    print 'Optimization'\n",
    "\n",
    "    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "    print \"%d train examples\" % len(train[0])\n",
    "    print \"%d valid examples\" % len(valid[0])\n",
    "    print \"%d test examples\" % len(test[0])\n",
    "\n",
    "    history_errs = []\n",
    "    best_p = None\n",
    "    bad_count = 0\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = len(train[0]) / batch_size\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = len(train[0]) / batch_size\n",
    "\n",
    "    uidx = 0  # the number of update done\n",
    "    estop = False  # early stop\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for eidx in xrange(max_epochs):\n",
    "            n_samples = 0\n",
    "\n",
    "            # Get new shuffled index for the training set.\n",
    "            kf = get_minibatches_idx(len(train[0]), batch_size, shuffle=True)\n",
    "\n",
    "            for _, train_index in kf:\n",
    "                uidx += 1\n",
    "                use_noise.set_value(1.)\n",
    "\n",
    "                # Select the random examples for this minibatch\n",
    "                y = [train[1][t] for t in train_index]\n",
    "                x = [train[0][t]for t in train_index]\n",
    "\n",
    "                # Get the data in numpy.ndarray format\n",
    "                # This swap the axis!\n",
    "                # Return something of shape (minibatch maxlen, n samples)\n",
    "                x, mask, y = prepare_data(x, y)\n",
    "                n_samples += x.shape[1]\n",
    "\n",
    "                cost = f_grad_shared(x, mask, y)\n",
    "                f_update(lrate)\n",
    "\n",
    "                if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                    print 'bad cost detected: ', cost\n",
    "                    return 1., 1., 1.\n",
    "\n",
    "                if numpy.mod(uidx, dispFreq) == 0:\n",
    "                    print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost\n",
    "\n",
    "                if saveto and numpy.mod(uidx, saveFreq) == 0:\n",
    "                    print 'Saving...',\n",
    "\n",
    "                    if best_p is not None:\n",
    "                        params = best_p\n",
    "                    else:\n",
    "                        params = unzip(tparams)\n",
    "                    numpy.savez(saveto, history_errs=history_errs, **params)\n",
    "                    pkl.dump(model_options, open('%s.pkl' % saveto, 'wb'), -1)\n",
    "                    print 'Done'\n",
    "\n",
    "                if numpy.mod(uidx, validFreq) == 0:\n",
    "                    use_noise.set_value(0.)\n",
    "                    train_err = pred_error(f_pred, prepare_data, train, kf)\n",
    "                    valid_err = pred_error(f_pred, prepare_data, valid,\n",
    "                                           kf_valid)\n",
    "                    test_err = pred_error(f_pred, prepare_data, test, kf_test)\n",
    "\n",
    "                    history_errs.append([valid_err, test_err])\n",
    "\n",
    "                    if (best_p is None or\n",
    "                        valid_err <= numpy.array(history_errs)[:,\n",
    "                                                               0].min()):\n",
    "\n",
    "                        best_p = unzip(tparams)\n",
    "                        bad_counter = 0\n",
    "\n",
    "                    print ('Train ', train_err, 'Valid ', valid_err,\n",
    "                           'Test ', test_err)\n",
    "\n",
    "                    if (len(history_errs) > patience and\n",
    "                        valid_err >= numpy.array(history_errs)[:-patience,\n",
    "                                                               0].min()):\n",
    "                        bad_counter += 1\n",
    "                        if bad_counter > patience:\n",
    "                            print 'Early Stop!'\n",
    "                            estop = True\n",
    "                            break\n",
    "\n",
    "            print 'Seen %d samples' % n_samples\n",
    "\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Training interupted\"\n",
    "\n",
    "    end_time = time.time()\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "    else:\n",
    "        best_p = unzip(tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    kf_train_sorted = get_minibatches_idx(len(train[0]), batch_size)\n",
    "    train_err = pred_error(f_pred, prepare_data, train, kf_train_sorted)\n",
    "    valid_err = pred_error(f_pred, prepare_data, valid, kf_valid)\n",
    "    test_err = pred_error(f_pred, prepare_data, test, kf_test)\n",
    "\n",
    "    print 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err\n",
    "    if saveto:\n",
    "        numpy.savez(saveto, train_err=train_err,\n",
    "                    valid_err=valid_err, test_err=test_err,\n",
    "                    history_errs=history_errs, **best_p)\n",
    "    print 'The code run for %d epochs, with %f sec/epochs' % (\n",
    "        (eidx + 1), (end_time - start_time) / (1. * (eidx + 1)))\n",
    "    print >> sys.stderr, ('Training took %.1fs' %\n",
    "                          (end_time - start_time))\n",
    "    return train_err, valid_err, test_err\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # See function train for all possible parameter and there definition.\n",
    "    train_lstm(\n",
    "        max_epochs=100,\n",
    "        test_size=500,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
