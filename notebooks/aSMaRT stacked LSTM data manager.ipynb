{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class character_data_manager:\n",
    "    \"\"\"Your data butler\"\"\"\n",
    "    def __init__(self,load_path,test_frac=.05,shuffle_scale=300,max_doc_loads=500,load_vec_flag=True):\n",
    "        # Store parameters\n",
    "        self.load_path = load_path\n",
    "        self.test_frac = test_frac\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.max_doc_loads = max_doc_loads\n",
    "        self.load_vec_flag = load_vec_flag\n",
    "        \n",
    "        # Load in the document dictionary, query_list, and the entity lookups\n",
    "        with open(load_path + 'document_dictionary.pkl','rb') as f:\n",
    "            self.doc_dict = cPickle.load(f)\n",
    "        with open(load_path + 'query_list.pkl','rb') as f:\n",
    "            self.query_list = cPickle.load(f)\n",
    "        with open(load_path + 'entity_vectors.pkl','rb') as f:\n",
    "            self.ent_vecs = cPickle.load(f)\n",
    "        with open(load_path + 'bad_entity_vectors.pkl','rb') as f:\n",
    "            self.bEnt_vecs = cPickle.load(f)\n",
    "            \n",
    "        # First things first, split the data\n",
    "        self.__split_train_test__()\n",
    "                \n",
    "        # Initialize a training schedule\n",
    "        self.__loaded_docs = []\n",
    "        self.__schedule_pos = 0\n",
    "        self.__initialize_training_schedule__()\n",
    "        \n",
    "    \n",
    "    # For doing a hard split of the data\n",
    "    def __split_train_test__(self):\n",
    "        # Randomly choose works and accumulate their queries until you have at least test_frac\n",
    "        train_works = self.doc_dict.keys()\n",
    "        \n",
    "        tot_queries = len(self.query_list)\n",
    "        n_test = 0\n",
    "        test_works   = []\n",
    "        test_queries = []\n",
    "        while float(len(test_queries)) / tot_queries < self.test_frac:\n",
    "            # Randomly add a work to the test_works\n",
    "            new_test_work = train_works.pop( np.random.randint(0,len(train_works)) )\n",
    "            test_works = test_works + [new_test_work]\n",
    "            \n",
    "            # Add all the queries for that work\n",
    "            test_queries += [q for q in self.query_list if q['doc'] == new_test_work]\n",
    "        \n",
    "        self.test_queries = test_queries\n",
    "        \n",
    "        # Re-create query_list so there is no overlap\n",
    "        self.query_list = [q for q in self.query_list if q['doc'] in train_works]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __initialize_training_schedule__(self):\n",
    "        self.__schedule_pos = 0\n",
    "        self.__weak_shuffle__()\n",
    "    \n",
    "    def __weak_shuffle__(self):\n",
    "        list_len = len(self.query_list)\n",
    "        def sortkey(x,obj):\n",
    "            list_len = len(obj.query_list)\n",
    "            x = x + np.random.normal(scale=obj.shuffle_scale)\n",
    "            if x < 0:\n",
    "                x = list_len - x\n",
    "            if x > list_len:\n",
    "                x = x - list_len\n",
    "            return x\n",
    "        \n",
    "        # Give the data a random circular shift\n",
    "        d = deque(self.query_list)\n",
    "        d.rotate(np.random.randint(0,list_len))\n",
    "        self.query_list = list(d)\n",
    "        \n",
    "        self.query_list =\\\n",
    "            [x for (y,x) in sorted( enumerate(self.query_list), key=lambda X: sortkey(X[0],self) )]    \n",
    "    \n",
    "    # We need a method to offer data. That's mostly what this is here for.\n",
    "    def offer_data(self):\n",
    "        \"\"\"Used to pull a data/answer pair from the manager\"\"\"\n",
    "        \n",
    "        # Isolate the relevant query \n",
    "        query = self.query_list[self.__schedule_pos]\n",
    "        \n",
    "        # The data will be in the form of a vector sequence\n",
    "        # So, we need to load and/or retreive this example's vector and tag sequences\n",
    "        # Retreive the document vectors\n",
    "        d_vec = self.__get_doc_vec__(query['doc'])\n",
    "        # Retreive the query vectors\n",
    "        q_vec = self.__get_query_vec__(query)\n",
    "        # Combine them\n",
    "        vecs = np.concatenate( (d_vec,q_vec) )\n",
    "        # Retreive the tags\n",
    "        tags = self.doc_dict[query['doc']]['tags'] + query['tags']\n",
    "        \n",
    "        # Store the vectors and tags so we can go right back to this point easily without re-loading\n",
    "        self.__current_vecs = vecs\n",
    "        self.__current_tags = tags\n",
    "        self.__current_ans  = query['a']\n",
    "        \n",
    "        # Apply the permutation procedure, deal with particular words, and return the info!\n",
    "        return self.permute_example()\n",
    "    \n",
    "    \n",
    "    # For managing the rather large data\n",
    "    def __get_doc_vec__(self,doc_name):\n",
    "        if type(self.doc_dict[doc_name]['vecs']) != type(None):\n",
    "            return self.doc_dict[doc_name]['vecs']\n",
    "        \n",
    "        else:\n",
    "            # Load the document vectors in\n",
    "            self.doc_dict[doc_name]['vecs'] = self.__get_vec__(self.doc_dict[doc_name])\n",
    "\n",
    "            self.__loaded_docs += [doc_name]\n",
    "\n",
    "            if len(self.__loaded_docs) > self.max_doc_loads:\n",
    "                # Reset the \"oldest\" load\n",
    "                self.doc_dict[self.__loaded_docs[0]]['vecs'] = None\n",
    "                self.__loaded_docs = self.__loaded_docs[1:]\n",
    "            \n",
    "            return self.doc_dict[doc_name]['vecs']\n",
    "    \n",
    "    def __get_query_vec__(self,query):\n",
    "        return self.__get_vec__(query)\n",
    "        \n",
    "    # The method for accessing vectors when we don't want to keep them in memory\n",
    "    def __get_vec__(self,dic):\n",
    "        if self.load_vec_flag:\n",
    "            # Load pre-computed vectors\n",
    "            with open(self.load_path + dic['loc'],'rb') as f:\n",
    "                return cPickle.load(f)\n",
    "            \n",
    "        else:\n",
    "            # Use spacy to compute vectors\n",
    "            # NOTE: THIS IS MUCH SLOWER !!!\n",
    "            return np.array([ t.vector for t in nlp(dic['text']) ])\n",
    "        \n",
    "            \n",
    "    # For permuting entities that ought not be memorized\n",
    "    def permute_example(self):\n",
    "        # Randomly permute good entities\n",
    "        e_perm = np.random.permutation( np.arange(1,self.ent_vecs.shape[0]) )\n",
    "        # And also bad entities\n",
    "        b_perm = np.random.permutation( np.arange(0,self.bEnt_vecs.shape[0]) )\n",
    "        \n",
    "        # Create a copy of the vectors\n",
    "        V = self.__current_vecs.copy()\n",
    "        \n",
    "        # Go through each token and make any final changes that are necessary\n",
    "        for i,t in enumerate(self.__current_tags):\n",
    "            if t:\n",
    "                if t[0] == 0:\n",
    "                    # An unknown word\n",
    "                    V[i,:] = self.unknown_vec()\n",
    "                if t[0] == 3:\n",
    "                    # A query start indicator\n",
    "                    V[i,:] = self.query_start_vec()\n",
    "                if t[0] == 4:\n",
    "                    # A blank (like, clue to the answer) indicator\n",
    "                    V[i,:] = self.ent_vecs[0,:]\n",
    "                if t[0] == 1:\n",
    "                    # An entity. Give it it's randomly assigned vector\n",
    "                    V[i,:] = self.ent_vecs[e_perm[t[1]-1],:]\n",
    "                if t[0] == 2:\n",
    "                    # A bad entity. Give it it's randomly assigned vector\n",
    "                    V[i,:] = self.bEnt_vecs[b_perm[t[1]-1],:]\n",
    "                    \n",
    "        # Return the resulting vector-sequence+answer pair\n",
    "        return V, e_perm[self.__current_ans-1]\n",
    "    \n",
    "    # For moving through the data\n",
    "    def advance_schedule(self):\n",
    "        self.__schedule_pos += 1\n",
    "        if self.__schedule_pos > len(self.query_list):\n",
    "            self.__initialize_training_schedule__()\n",
    "            \n",
    "            \n",
    "    # Vectors for things we have to make up\n",
    "    def unknown_vec(self):\n",
    "        return np.zeros(300).astype('float32')\n",
    "    \n",
    "    def query_start_vec(self):\n",
    "        O = np.ones(300).astype('float32')\n",
    "        return O / np.sqrt(300.)\n",
    "    \n",
    "    \n",
    "    # For getting to know your butler\n",
    "    def num_loaded(self):\n",
    "        return len(self.__loaded_docs)\n",
    "    \n",
    "    def loaded_docs(self):\n",
    "        return self.__loaded_docs[:]\n",
    "    \n",
    "    def vec_memory_footprint(self):\n",
    "        D = 0\n",
    "        for k,v in self.doc_dict.iteritems():\n",
    "            try:\n",
    "                nB = v['vecs'].nbytes\n",
    "            except:\n",
    "                nB = sys.getsizeof(v['vecs'])\n",
    "            D += nB\n",
    "        return D / 1e6\n",
    "    \n",
    "    def get_current_query(self):\n",
    "        return self.query_list[self.__schedule_pos]\n",
    "    def get_current_doc(self):\n",
    "        return self.doc_dict[self.query_list[self.__schedule_pos]['doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_butler = character_data_manager('/Users/alex/Desktop/data manager/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}